---
title: "Section 1.1"
author: "David Nguyen"
date: "February 2, 2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(binom)
```

The following reproduces some parts of STAT 875 lecture notes for section 1.1

# The binomial distribution

The PMF of the Bernoulli distribution is $P(Y=y) = \pi^y(1-\pi)^{1-y}$ for $y=0$ or $1$.

The binomial distribution is the sum of $n$ independent Bernoulli trials ($W = \sum_{i=1}^n Y_i$) and has PMF:

\begin{align}
P(W=w) & =\frac{n!}{w!(n-w)!}  \pi^w(1-\pi)^{1-w} \\
       & = {n \choose w}  \pi^w(1-\pi)^{1-w}
\end{align}

* $W$ is a random variable representing the number of successes out of $n$ trials
* $W = 0, 1, \ldots, n$
* $n$ is a fixed constant
* $\pi \in [0,1]$ is the probability of success 

Mean $E(W) = n\pi$ 

Variance $Var(W) = n\pi(1-\pi)$

# Example - field goals
Suppose a field goal kicker attempts 5 field goals during a game and each field goal has the same probability of being successful (the kick is made). Also, assume each field goal is attempted under similar conditions; i.e., distance, weather, surface,â€¦. 

Suppose $n = 5$ and $\pi = 0.6$

What are the probabilities of getting $0, 1, 2, \ldots, or 5$ goals?

```{r}
pmf_goals <- data.frame(goals = 0:5,
          probability = dbinom(x = 0:5, size = 5, prob = 0.6))

pmf_goals %>%
  knitr::kable(digits = 2)

pmf_goals %>%
  ggplot(aes(x = goals, y = probability)) +
  geom_point(size = 2) + geom_segment(aes(xend = goals, yend = 0))  +  
  ylim(0, 0.4) +
  labs(title = expression(paste("PMF for Binomial(", pi, " = 0.6, n = 5)")), 
       y = "P(W = w)",
       x = "goals (w)")
```

# Maximum likelihood estimation

For an iid sample of $\boldsymbol y = y_1, y_2, \ldots, y_n \sim Binomial(\pi, n)$ the maximum likelihood estimate of $\pi$ is:

$$\hat \pi = \frac{\sum_{i=1}^n y_i}{n}$$

Note that the estimate $\hat \pi$ will vary from sample to sample, that is, it is a realizatin of a random variable. In general, maximum likelihood estimators $\hat \theta$ are (asymptotically) approximately normal

$$\hat \theta \overset{.}{\sim} N(\theta, Var(\hat \theta))$$

where $Var(\hat \theta)) = - \left[ E \left( \frac{\partial^2 \ln(L( \theta | \boldsymbol y ))}{\partial\theta^2 } \right) \right]^{-1} \Biggr|_{\theta = \hat \theta}$

For the binomial distribution, $Var(\hat\pi) = \frac{\hat \pi (1 - \hat \pi)}{n}$.

# Wald confidence interval

This interval exploits the asymptotic normality of $\hat \pi$ to get confidence intervals using normal quantiles. Standardizing $\hat \pi$ yields

$$\frac{\hat \pi - \pi}{\sqrt  {Var(\hat \theta)}} \overset{.}{\sim} N(0,1)$$
Because of this we can say that 

\begin{align}
P \left(Z_{\alpha/2} < \frac{\hat \pi - \pi}{\sqrt  {Var(\hat \theta)}} < Z_{1- \alpha /2} \right) \approx 1 - \alpha \\
P \left(\hat \theta -  Z_{1 - \alpha /2} Var(\hat \theta) < \theta < \hat \theta Z_{\alpha /2} Var(\hat \theta) \right) \approx 1 - \alpha
\end{align}

So the Wald $(1-\alpha)100\%$ CI for $\theta$ is:

\begin{align} 
\hat \theta \pm \text{(distributional value)(standard deviation of } \theta) \\
\hat \pi \pm Z_{1-\alpha/2}\sqrt{\frac{\hat\pi(1-\hat\pi)}{n}} \text{ for the binomial probability of success}
\end{align}

```{r}
# from binom package

```

